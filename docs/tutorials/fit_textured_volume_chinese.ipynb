{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This tutorial shows how to fit a volume given a set of views of a scene using differentiable volumetric rendering.\n",
    "\n",
    "More specificially, this tutorial will explain how to:\n",
    "1. 如何创建一个可微分的立体渲染器.\n",
    "2. 如何创建一个立体模型（包括如何使用 Volumes 类）\n",
    "3. 使用可微分的立体渲染器，根据图像拟合立体结构\n",
    "4. 将预测的立体结构可视化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 安装和导入模块\n",
    "确保已安装 torch 和 torchvision。如果没有安装 pytorch3d，请使用以下代码安装。:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "need_pytorch3d=False\n",
    "try:\n",
    "    import pytorch3d\n",
    "except ModuleNotFoundError:\n",
    "    need_pytorch3d=True\n",
    "if need_pytorch3d:\n",
    "    if torch.__version__.startswith(\"1.7\") and sys.platform.startswith(\"linux\"):\n",
    "        # 通过发布的 wheel 软件包安装 PyTorch3D\n",
    "        version_str=\"\".join([\n",
    "            f\"py3{sys.version_info.minor}_cu\",\n",
    "            torch.version.cuda.replace(\".\",\"\"),\n",
    "            f\"_pyt{torch.__version__[0:5:2]}\"\n",
    "        ])\n",
    "        !pip install pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
    "    else:\n",
    "        # 通过源码安装 PyTorch3D\n",
    "        !curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz\n",
    "        !tar xzf 1.10.0.tar.gz\n",
    "        os.environ[\"CUB_HOME\"] = os.getcwd() + \"/cub-1.10.0\"\n",
    "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "\n",
    "# 用于渲染的数据结构和函数\n",
    "from pytorch3d.structures import Volumes\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, \n",
    "    VolumeRenderer,\n",
    "    NDCGridRaysampler,\n",
    "    EmissionAbsorptionRaymarcher\n",
    ")\n",
    "from pytorch3d.transforms import so3_exponential_map\n",
    "\n",
    "# 为 demo utils 函数添加路径 \n",
    "sys.path.append(os.path.abspath(''))\n",
    "from utils.plot_image_grid import image_grid\n",
    "from utils.generate_cow_renders import generate_cow_renders\n",
    "\n",
    "# 准备可用设备\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 生成场景及 mask 的图像\n",
    "\n",
    "The following cell generates our training data.\n",
    "It renders the cow mesh from the `fit_textured_mesh.ipynb` tutorial from several viewpoints and returns:\n",
    "1. A batch of image and silhouette tensors that are produced by the cow mesh renderer.\n",
    "2. A set of cameras corresponding to each render.\n",
    "\n",
    "Note: For the purpose of this tutorial, which aims at explaining the details of volumetric rendering, we do not explain how the mesh rendering, implemented in the `generate_cow_renders` function, works. Please refer to `fit_textured_mesh.ipynb` for a detailed explanation of mesh rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cameras, target_images, target_silhouettes = generate_cow_renders(num_views=40)\n",
    "print(f'Generated {len(target_images)} images/silhouettes/cameras.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 初始化体积渲染器\n",
    "\n",
    "The following initializes a volumetric renderer that emits a ray from each pixel of a target image and samples a set of uniformly-spaced points along the ray. At each ray-point, the corresponding density and color value is obtained by querying the corresponding location in the volumetric model of the scene (the model is described & instantiated in a later cell).\n",
    "\n",
    "The renderer is composed of a *raymarcher* and a *raysampler*.\n",
    "- The *raysampler* is responsible for emiting rays from image pixels and sampling the points along them. Here, we use the `NDCGridRaysampler` which follows the standard PyTorch3D coordinate grid convention (+X from right to left; +Y from bottom to top; +Z away from the user).\n",
    "- The *raymarcher* takes the densities and colors sampled along each ray and renders each ray into a color and an opacity value of the ray's source pixel. Here we use the `EmissionAbsorptionRaymarcher` which implements the standard Emission-Absorption raymarching algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render_size 表示渲染图像各个边的像素大小，将其设置为与目标图像尺寸一致 \n",
    "# 也就是说将其渲染成与基准图像一样的尺寸\n",
    "# 渲染场景以（0,0,0）为中心，被限定在一个边长约等于 3.0 (国际单位)的边框内\n",
    "render_size = target_images.shape[1]\n",
    "\n",
    "volume_extent_world = 3.0\n",
    "\n",
    "# 1) 实例化 raysampler\n",
    "# 此处 NDCGridRaysampler 会生成一矩形图像网格的射线\n",
    "# 其坐标遵循 pytorch3d 坐标规定\n",
    "# 大致相当于每个体素都有一个射线点\n",
    "# 由于此处设定的体积是 128^3，因此取样 n_pts_per_ray=150\n",
    "# 进一步设置 min_depth=0.1，因为相机平面内的所有表面都超过了 0.1 单位\n",
    "raysampler = NDCGridRaysampler(\n",
    "    image_width=render_size,\n",
    "    image_height=render_size,\n",
    "    n_pts_per_ray=150,\n",
    "    min_depth=0.1,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "\n",
    "\n",
    "# 2) 实例化 raymarcher.\n",
    "# 此处用的是标准 EmissionAbsorptionRaymarcher\n",
    "# 它会沿着每条射线前进\n",
    "# 将每条射线都渲染成一个单一的 3D 颜色向量和一个不透明度标量\n",
    "raymarcher = EmissionAbsorptionRaymarcher()\n",
    "\n",
    "# 最后，用 raysampler 和 raymarcher 实例化体积渲染器\n",
    "renderer = VolumeRenderer(\n",
    "    raysampler=raysampler, raymarcher=raymarcher,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 初始化体积模型\n",
    "\n",
    "接下来实例化场景的体积模型。这会使得 3D 空间量化为体积像素，其中每个体素都用体素 RGB 颜色的 3D 向量，和描述体素不透明度的密度标量（范围在[0-1]之间，数字越大不透明越高）来表示.\n",
    "\n",
    "为了保证密度和颜色的取值范围在 [0-1] 之间，我们会在对数空间中表示体积颜色和密度。模型运行 forward 函数时，log-space 值会通过 sigmoid 函数传递，从而使得 log-space 值处于正确的取值范围。\n",
    "\n",
    "此外， VolumeModel 还包含渲染器对象。这个对象在整个优化过程中保持不变。\n",
    "\n",
    "本部分代码还定义了 huber 损失函数，它可以计算出渲染色和 mask 之间的差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumeModel(torch.nn.Module):\n",
    "    def __init__(self, renderer, volume_size=[64] * 3, voxel_size=0.1):\n",
    "        super().__init__()\n",
    "        # 对 torch.sigmoid(self.log_colors)进行评估后\n",
    "        # 得到的密度值接近于 0\n",
    "        self.log_densities = torch.nn.Parameter(-4.0 * torch.ones(1, *volume_size))\n",
    "        # 对 torch.sigmoid(self.log_colors)进行评估后 \n",
    "        # 图像变为中性灰色\n",
    "        self.log_colors = torch.nn.Parameter(torch.zeros(3, *volume_size))\n",
    "        self._voxel_size = voxel_size\n",
    "        # 同时存储渲染器模块\n",
    "        self._renderer = renderer\n",
    "        \n",
    "    def forward(self, cameras):\n",
    "        batch_size = cameras.R.shape[0]\n",
    "\n",
    "        # 将 log-space 值转换为密度/颜色\n",
    "        densities = torch.sigmoid(self.log_densities)\n",
    "        colors = torch.sigmoid(self.log_colors)\n",
    "        \n",
    "        # 实例化体积对象，地扩展了 batch_size-times\n",
    "        # 确保密度和颜色准确无误\n",
    "        # 扩展了 batch_size-times\n",
    "        volumes = Volumes(\n",
    "            densities = densities[None].expand(\n",
    "                batch_size, *self.log_densities.shape),\n",
    "            features = colors[None].expand(\n",
    "                batch_size, *self.log_colors.shape),\n",
    "            voxel_size=self._voxel_size,\n",
    "        )\n",
    "        \n",
    "        # 给定相机和体积\n",
    "        # 运行渲染器并只返回第一个输出值\n",
    "        # (第二个输出表示采样射线\n",
    "        # 此处可以忽略).\n",
    "        return self._renderer(cameras=cameras, volumes=volumes)[0]\n",
    "    \n",
    "# 辅助函数，用于评估\n",
    "# 渲染的轮廓和颜色之间的 smooth L1（huber）损失\n",
    "def huber(x, y, scaling=0.1):\n",
    "    diff_sq = (x - y) ** 2\n",
    "    loss = ((1 + diff_sq / (scaling**2)).clamp(1e-4).sqrt() - 1) * float(scaling)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 体积拟合\n",
    "\n",
    "这一步，我们用可微分渲染来进行体积拟合\n",
    "\n",
    "为了拟合体积，我们从 target_camera 的视角进行渲染\n",
    "并将渲染结果与观察到的 target_images 和 target_silhouettes 进行对比\n",
    "\n",
    "这种对比是通过评估的 target_images/rendered_images 和\n",
    " target_silhouettes/rendered_silhouettes 之间的平均 huber（smooth-l1）误差来完成的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先将所有相关变量移至正确的设备上\n",
    "target_cameras = target_cameras.to(device)\n",
    "target_images = target_images.to(device)\n",
    "target_silhouettes = target_silhouettes.to(device)\n",
    "\n",
    "# 实例化体积模型\n",
    "# 此处使用的是一个 \n",
    "# 边长= 128 的立方体的体积\n",
    "# 体积中每个体素的大小设置为 volume_extent_world/volume_size s.t.\n",
    "# 该体积表示以(0，0，0)为中心的 3D 边框所包围的空间\n",
    "# 每个边的大小为 3\n",
    "volume_size = 128\n",
    "volume_model = VolumeModel(\n",
    "    renderer,\n",
    "    volume_size=[volume_size] * 3, \n",
    "    voxel_size = volume_extent_world / volume_size,\n",
    ").to(device)\n",
    "\n",
    "# 实例化 Adam 优化器。将学习率设置为 0.1\n",
    "lr = 0.1\n",
    "optimizer = torch.optim.Adam(volume_model.parameters(), lr=lr)\n",
    "\n",
    "# 将 Adam 迭代 300，并在每个 minibatch 中随机抽取 10 张图像\n",
    "batch_size = 10\n",
    "n_iter = 300\n",
    "for iteration in range(n_iter):\n",
    "\n",
    "    # 运行完 75% 的迭代次数时\n",
    "    # 将优化器的学习率降低 10 倍\n",
    "    if iteration == round(n_iter * 0.75):\n",
    "        print('Decreasing LR 10-fold ...')\n",
    "        optimizer = torch.optim.Adam(\n",
    "            volume_model.parameters(), lr=lr * 0.1\n",
    "        )\n",
    "    \n",
    "    # 将优化器梯度设置为零\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 随机批次指数取样\n",
    "    batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n",
    "    \n",
    "    # 相机 minibatch 取样\n",
    "    batch_cameras = FoVPerspectiveCameras(\n",
    "        R = target_cameras.R[batch_idx], \n",
    "        T = target_cameras.T[batch_idx], \n",
    "        znear = target_cameras.znear[batch_idx],\n",
    "        zfar = target_cameras.zfar[batch_idx],\n",
    "        aspect_ratio = target_cameras.aspect_ratio[batch_idx],\n",
    "        fov = target_cameras.fov[batch_idx],\n",
    "        device = device,\n",
    "    )\n",
    "    \n",
    "    # 评估体积模型\n",
    "    rendered_images, rendered_silhouettes = volume_model(\n",
    "        batch_cameras\n",
    "    ).split([3, 1], dim=-1)\n",
    "    \n",
    "    # 计算轮廓误差\n",
    "    # 并将其作为预测 mask\n",
    "    # 和目标轮廓之间的平均 huber 损失\n",
    "    sil_err = huber(\n",
    "        rendered_silhouettes[..., 0], target_silhouettes[batch_idx],\n",
    "    ).abs().mean()\n",
    "\n",
    "    # 计算颜色误差\n",
    "    # 并将其作为渲染颜色\n",
    "    # 和基准图像之间的平均 huber 损失\n",
    "    color_err = huber(\n",
    "        rendered_images, target_images[batch_idx],\n",
    "    ).abs().mean()\n",
    "    \n",
    "    # 优化损失是颜色和\n",
    "    # 轮廓误差的简单相加\n",
    "    loss = color_err + sil_err \n",
    "    \n",
    "    # 打印当前的损失值\n",
    "    if iteration % 10 == 0:\n",
    "        print(\n",
    "            f'Iteration {iteration:05d}:'\n",
    "            + f' color_err = {float(color_err):1.2e}'\n",
    "            + f' mask_err = {float(sil_err):1.2e}'\n",
    "        )\n",
    "    \n",
    "    # 继续优化\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 每跑完 40 次迭代，进行一次可视化渲染\n",
    "    if iteration % 40 == 0:\n",
    "        # Visualize only a single randomly selected element of the batch.\n",
    "        im_show_idx = int(torch.randint(low=0, high=batch_size, size=(1,)))\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "        ax = ax.ravel()\n",
    "        clamp_and_detach = lambda x: x.clamp(0.0, 1.0).cpu().detach().numpy()\n",
    "        ax[0].imshow(clamp_and_detach(rendered_images[im_show_idx]))\n",
    "        ax[1].imshow(clamp_and_detach(target_images[batch_idx[im_show_idx], ..., :3]))\n",
    "        ax[2].imshow(clamp_and_detach(rendered_silhouettes[im_show_idx, ..., 0]))\n",
    "        ax[3].imshow(clamp_and_detach(target_silhouettes[batch_idx[im_show_idx]]))\n",
    "        for ax_, title_ in zip(\n",
    "            ax, \n",
    "            (\"rendered image\", \"target image\", \"rendered silhouette\", \"target silhouette\")\n",
    "        ):\n",
    "            ax_.grid(\"off\")\n",
    "            ax_.axis(\"off\")\n",
    "            ax_.set_title(title_)\n",
    "        fig.canvas.draw(); fig.show()\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 将优化后的场景体积进行可视化",
    "\n",
    "最后，旋转场景体积的 y 轴，从多个视点进行渲染，将优化后的体积进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rotating_volume(volume_model, n_frames = 50):\n",
    "    logRs = torch.zeros(n_frames, 3, device=device)\n",
    "    logRs[:, 1] = torch.linspace(0.0, 2.0 * 3.14, n_frames, device=device)\n",
    "    Rs = so3_exponential_map(logRs)\n",
    "    Ts = torch.zeros(n_frames, 3, device=device)\n",
    "    Ts[:, 2] = 2.7\n",
    "    frames = []\n",
    "    print('Generating rotating volume ...')\n",
    "    for R, T in zip(tqdm(Rs), Ts):\n",
    "        camera = FoVPerspectiveCameras(\n",
    "            R=R[None], \n",
    "            T=T[None], \n",
    "            znear = target_cameras.znear[0],\n",
    "            zfar = target_cameras.zfar[0],\n",
    "            aspect_ratio = target_cameras.aspect_ratio[0],\n",
    "            fov = target_cameras.fov[0],\n",
    "            device=device,\n",
    "        )\n",
    "        frames.append(volume_model(camera)[..., :3].clamp(0.0, 1.0))\n",
    "    return torch.cat(frames)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    rotating_volume_frames = generate_rotating_volume(volume_model, n_frames=7*4)\n",
    "\n",
    "image_grid(rotating_volume_frames.clamp(0., 1.).cpu().numpy(), rows=4, cols=7, rgb=True, fill=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 结论\n",
    "\n",
    "本教程中演示了如何优化场景的 3D 立体构造，使已知视点的体积渲染与每个视点的观测图像相匹配。教程中的渲染是使用 NDCGridRaysampler 和 EmissionAbsorptionRaymarcher 构成的 PyTorch3D 立体渲染器完成的。"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "kernelspec": {
   "display_name": "pytorch3d_etc (local)",
   "language": "python",
   "name": "pytorch3d_etc_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
